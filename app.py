import matplotlib
matplotlib.use('Agg') # è§£å†³ Streamlit Cloud çš„æ— ç•Œé¢æ¸²æŸ“é”™è¯¯

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback
import json, imageio, os
from datetime import datetime
import hashlib
import time # ç”¨äº render æ–¹æ³•ä¸­çš„æ½œåœ¨ä¼˜åŒ–

# ---------------------------------
# Streamlit/Colab å…¼å®¹çš„æ‰“å°å‡½æ•° (æ›¿ä»£ st.write)
# ---------------------------------
def print_status(message):
    """Prints status messages, checking if running in Streamlit."""
    if 'streamlit' in st.__doc__:
        st.info(message)
    else:
        print(message)

# ---------------------------------
# è®­ç»ƒæ—¥å¿—å›è°ƒå‡½æ•°
# ---------------------------------
class LoggingCallback(BaseCallback):
    """
    Saves training metrics to a list for POUW inclusion.
    """
    def __init__(self, verbose=0):
        super(LoggingCallback, self).__init__(verbose)
        self.logs = []

    def _on_step(self) -> bool:
        # æ¯ 1000 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
        if self.n_calls % 1000 == 0:
            avg_reward = np.mean(self.locals['rewards']) if len(self.locals['rewards']) > 0 else 0
            self.logs.append({
                'timesteps': self.num_timesteps,
                'avg_reward': float(f'{avg_reward:.2f}')
            })
            if self.verbose > 0:
                print(f"Step {self.num_timesteps}/{self.locals['total_timesteps']} | Avg Reward: {avg_reward:.2f}")
        return True

# ---------------------------------
# 2. RL ç¯å¢ƒå®šä¹‰ (SmartLogisticsNavEnv)
# ---------------------------------
class SmartLogisticsNavEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(self, grid_size=20, mode='shortest', render_mode=None):
        super(SmartLogisticsNavEnv, self).__init__()
        self.grid_size = grid_size
        self.mode = mode
        
        # éšœç¢ç‰©ä½ç½® (ç¤ºä¾‹ï¼šä¸€æ¡å¯¹è§’çº¿éšœç¢)
        self.obstacles = [(i, i) for i in range(5, grid_size - 5)]
        
        # åŠ¨ä½œç©ºé—´ï¼š0: ä¸Š, 1: ä¸‹, 2: å·¦, 3: å³
        self.action_space = spaces.Discrete(4)
        
        # è§‚æµ‹ç©ºé—´ï¼š(agent_x, agent_y, target_x, target_y)
        low = np.array([0, 0, 0, 0], dtype=np.int32)
        high = np.array([grid_size - 1, grid_size - 1, grid_size - 1, grid_size - 1], dtype=np.int32)
        self.observation_space = spaces.Box(low=low, high=high, shape=(4,), dtype=np.int32)
        
        # ç›®æ ‡ä½ç½® (å³ä¸‹è§’) å’Œèµ·å§‹ä½ç½® (å·¦ä¸Šè§’)
        self.target_pos = (grid_size - 1, grid_size - 1)
        self.start_pos = (0, 0)
        self.agent_pos = self.start_pos
        self.current_step = 0
        self.max_steps = grid_size * grid_size * 2
        
        self.render_mode = render_mode
        self.window = None
        self.clock = None

    def _get_obs(self):
        return np.array([self.agent_pos[0], self.agent_pos[1], 
                         self.target_pos[0], self.target_pos[1]])

    def _calculate_reward(self, prev_dist):
        reward = 0
        
        # 1. è·ç¦»å¥–åŠ± (Progress)
        current_dist = self._calculate_distance()
        distance_change = prev_dist - current_dist
        
        # 2. æ¨¡å¼ç‰¹å®šçš„å¥–åŠ±/æƒ©ç½š
        if self.mode == 'shortest':
            # é¼“åŠ±é è¿‘ç›®æ ‡
            reward += distance_change * 10
            # å°æƒ©ç½šæ—¶é—´æ­¥é•¿ï¼Œé¼“åŠ±å¿«é€Ÿç»“æŸ
            reward -= 0.1
        elif self.mode == 'fastest':
            # é¼“åŠ±å¿«é€Ÿé è¿‘ (æ›´é«˜çš„è·ç¦»å¥–åŠ±)
            reward += distance_change * 20
            # è¾ƒé«˜æƒ©ç½šæ—¶é—´æ­¥é•¿
            reward -= 0.5
        elif self.mode == 'balanced':
            # å‡è¡¡å¥–åŠ± (è·ç¦»å’Œæ—¶é—´)
            reward += distance_change * 5
            reward -= 0.2
        
        # 3. ç¢°æ’æƒ©ç½š (Punishment)
        if self.agent_pos in self.obstacles:
            reward -= 1000  # æé«˜çš„ç¢°æ’æƒ©ç½š

        # 4. è¾¹ç•Œæƒ©ç½š
        if not (0 <= self.agent_pos[0] < self.grid_size and 0 <= self.agent_pos[1] < self.grid_size):
            reward -= 50  # è¾¹ç•Œæƒ©ç½š (RL Agent ä¸åº”è¯¥è¶Šç•Œ)

        # 5. èƒœåˆ©å¥–åŠ± (Goal)
        if self.agent_pos == self.target_pos:
            reward += 10000  # æé«˜çš„èƒœåˆ©å¥–åŠ±
            
        return reward

    def _calculate_distance(self):
        # æ›¼å“ˆé¡¿è·ç¦»
        return abs(self.agent_pos[0] - self.target_pos[0]) + abs(self.agent_pos[1] - self.target_pos[1])

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.agent_pos = self.start_pos
        self.current_step = 0
        
        observation = self._get_obs()
        info = self._get_info()
        return observation, info

    def step(self, action):
        prev_dist = self._calculate_distance()
        x, y = self.agent_pos
        
        # ç§»åŠ¨ (0:ä¸Š, 1:ä¸‹, 2:å·¦, 3:å³)
        if action == 0: y = min(y + 1, self.grid_size - 1)
        elif action == 1: y = max(y - 1, 0)
        elif action == 2: x = max(x - 1, 0)
        elif action == 3: x = min(x + 1, self.grid_size - 1)
        
        self.agent_pos = (x, y)
        self.current_step += 1
        
        reward = self._calculate_reward(prev_dist)
        
        terminated = self.agent_pos == self.target_pos
        truncated = self.current_step >= self.max_steps

        observation = self._get_obs()
        info = self._get_info()
        
        return observation, reward, terminated, truncated, info

    def _get_info(self):
        return {"distance": self._calculate_distance(), "agent_pos": self.agent_pos}

    # æ¸²æŸ“æ–¹æ³• (ç”¨äº GIF ç”Ÿæˆ)
    def render(self):
        grid = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.uint8)
        
        # éšœç¢ç‰© (çº¢è‰²)
        for x, y in self.obstacles:
            if 0 <= x < self.grid_size and 0 <= y < self.grid_size:
                grid[y, x] = [255, 0, 0]
        
        # ç›®æ ‡ (ç»¿è‰²)
        gx, gy = self.target_pos
        grid[gy, gx] = [0, 255, 0]
        
        # Agent (è“è‰²)
        ax, ay = self.agent_pos
        if 0 <= ax < self.grid_size and 0 <= ay < self.grid_size:
             grid[ay, ax] = [0, 0, 255]
        
        # ä½¿ç”¨ matplotlib ç»˜åˆ¶ç½‘æ ¼
        fig, ax = plt.subplots(figsize=(self.grid_size/2, self.grid_size/2))
        ax.imshow(grid, origin='lower')
        ax.set_xticks(np.arange(self.grid_size))
        ax.set_yticks(np.arange(self.grid_size))
        ax.set_xticklabels([])
        ax.set_yticklabels([])
        ax.tick_params(length=0) 
        
        # --- å›¾åƒç”Ÿæˆä¿®å¤ç‚¹ ---
        fig.canvas.draw()
        
        # å°è¯•åˆ·æ–°äº‹ä»¶ï¼Œç¡®ä¿æ¸²æŸ“å®Œæˆï¼ˆå°¤å…¶åœ¨æ— ç•Œé¢ç¯å¢ƒä¸­ï¼‰
        try:
             fig.canvas.flush_events() 
        except NotImplementedError:
             pass
        
        # è·å– RGB æ•°æ®
        width, height = fig.canvas.get_width_height()
        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
        
        # ç¡®ä¿å°ºå¯¸æ­£ç¡® (height x width x 3)
        image = image.reshape(height, width, 3) 
        plt.close(fig) 
        # --- ä¿®å¤ç‚¹ç»“æŸ ---
        
        return image

# ---------------------------------
# 3. POUW åŒºå—é“¾é€»è¾‘ (Block & SimpleBlockchain)
# ---------------------------------
class Block:
    def __init__(self, index, timestamp, data, previous_hash='0'):
        self.index = index
        self.timestamp = timestamp
        self.data = data
        self.previous_hash = previous_hash
        self.nonce = 0
        self.hash = self.calculate_hash()

    def calculate_hash(self):
        block_string = json.dumps(self.to_dict(), sort_keys=True).encode()
        return hashlib.sha256(block_string).hexdigest()

    def to_dict(self):
        return {
            "index": self.index,
            "timestamp": self.timestamp,
            "data": self.data,
            "previous_hash": self.previous_hash,
            "nonce": self.nonce
        }

class SimpleBlockchain:
    def __init__(self):
        self.chain = [self.create_genesis_block()]
        self.difficulty = 4 # æŒ–çŸ¿éš¾åº¦ï¼šå“ˆå¸Œå¿…é¡»ä»¥ 4 ä¸ªé›¶å¼€å¤´

    def create_genesis_block(self):
        # åˆ›ä¸–åŒºå—ä½¿ç”¨å›ºå®šçš„ POUW æ•°æ®
        return Block(0, str(datetime.now()), 
                     {"message": "Genesis Block for RL POUW Chain"}, "0")

    def get_latest_block(self):
        return self.chain[-1]

    def mine_block(self, new_block):
        # æ‰¾åˆ°æ»¡è¶³éš¾åº¦è¦æ±‚çš„ Nonce
        target = '0' * self.difficulty
        while new_block.hash[:self.difficulty] != target:
            new_block.nonce += 1
            new_block.hash = new_block.calculate_hash()
        
        # å°†æŒ–å‡ºçš„åŒºå—æ·»åŠ åˆ°é“¾ä¸­
        self.chain.append(new_block)
        return new_block.hash

    def is_chain_valid(self):
        for i in range(1, len(self.chain)):
            current_block = self.chain[i]
            previous_block = self.chain[i-1]
            
            # 1. æ£€æŸ¥å“ˆå¸Œæ˜¯å¦æ­£ç¡®è®¡ç®— (é˜²æ­¢æ•°æ®è¢«ç¯¡æ”¹)
            if current_block.hash != current_block.calculate_hash():
                return False
            
            # 2. æ£€æŸ¥é“¾çš„è¿æ¥æ˜¯å¦æ­£ç¡® (é˜²æ­¢åŒºå—é¡ºåºè¢«ç¯¡æ”¹)
            if current_block.previous_hash != previous_block.hash:
                return False
                
            # 3. æ£€æŸ¥å“ˆå¸Œæ˜¯å¦æ»¡è¶³æŒ–çŸ¿éš¾åº¦
            target = '0' * self.difficulty
            if current_block.hash[:self.difficulty] != target:
                return False
                
        return True

# ---------------------------------
# 4. æ ¸å¿ƒè®­ç»ƒå’Œè¯„ä¼°å‡½æ•°
# ---------------------------------

def train_agent(mode, timesteps, grid_size=20):
    """è®­ç»ƒ PPO Agent å¹¶è¿”å›æ¨¡å‹å’Œæ—¥å¿—"""
    print_status(f"Training PPO Agent for {mode} mode with {timesteps} steps...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = SmartLogisticsNavEnv(grid_size=grid_size, mode=mode)
    vec_env = make_vec_env(lambda: env, n_envs=1)
    
    # åˆ›å»ºæ—¥å¿—å›è°ƒ
    logging_callback = LoggingCallback(verbose=1)

    # åˆå§‹åŒ– PPO æ¨¡å‹
    model = PPO("MlpPolicy", vec_env, verbose=0, device="auto", tensorboard_log=None)
    
    # è®­ç»ƒ
    model.learn(total_timesteps=timesteps, callback=logging_callback)
    
    # ä¿å­˜æ¨¡å‹ (å¯é€‰)
    model.save(f"ppo_logistics_{mode}.zip")
    
    return model, logging_callback.logs

def run_test_and_render(model, mode, grid_size=20):
    """è¯„ä¼°æ¨¡å‹å¹¶ç”Ÿæˆå¯¼èˆª GIF"""
    print_status("Running Final Evaluation...")
    
    env = SmartLogisticsNavEnv(grid_size=grid_size, mode=mode)
    obs, _ = env.reset()
    
    images = []
    total_reward = 0
    steps = 0
    done = False
    
    while not done and steps < env.max_steps:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        total_reward += reward
        steps += 1
        
        # è®°å½•å›¾åƒå¸§
        images.append(env.render())
        
    env.close()
    
    # ä¿å­˜ GIF
    gif_path = f"navigation_{mode}.gif"
    imageio.mimsave(gif_path, images, fps=10)
    
    test_result = {
        'steps': steps,
        'total_reward': total_reward,
        'reach_goal': env.agent_pos == env.target_pos
    }
    
    return test_result, gif_path

def save_pouw_to_blockchain(user_params, training_logs, test_result, model):
    """å°† RL è®­ç»ƒç»“æœä½œä¸º POUW æ•°æ®è®°å½•åˆ°åŒºå—é“¾"""
    # ç»„åˆ POUW æ•°æ®
    pouw_data = {
        "user_params": user_params,
        "training_summary": {
            "start_time": str(datetime.now()),
            "total_timesteps": user_params['total_timesteps'],
            "final_reward": training_logs[-1]['avg_reward'] if training_logs else 0
        },
        "test_result": test_result,
        "model_architecture": str(model.policy.net)
    }
    
    # è·å–æœ€æ–°åŒºå—
    latest_block = st.session_state.rl_pouw_chain.get_latest_block()
    
    # åˆ›å»ºæ–°åŒºå—
    new_index = latest_block.index + 1
    new_block = Block(new_index, str(datetime.now()), pouw_data, latest_block.hash)
    
    # æŒ–çŸ¿å¹¶æ·»åŠ åˆ°é“¾
    mined_hash = st.session_state.rl_pouw_chain.mine_block(new_block)
    
    # éªŒè¯é“¾çš„æœ‰æ•ˆæ€§
    chain_valid = st.session_state.rl_pouw_chain.is_chain_valid()

    return {
        "block_index": new_block.index,
        "block_hash": mined_hash,
        "data": pouw_data,
        "is_chain_valid": chain_valid
    }

# ---------------------------------
# 5. Streamlit Web App Interface
# ---------------------------------

# åˆå§‹åŒ– Streamlit çŠ¶æ€å’ŒåŒºå—é“¾ (å¿…é¡»åœ¨æ‰€æœ‰ Streamlit å…ƒç´ ä¹‹å‰)
if 'rl_pouw_chain' not in st.session_state:
    st.session_state.rl_pouw_chain = SimpleBlockchain()

st.set_page_config(layout="wide")
st.title("ğŸ¤– RL-POUW æ™ºèƒ½ç‰©æµå¯¼èˆª MVP")
st.markdown("---")

# UI controls and inputs
col1, col2, col3 = st.columns(3)
with col1:
    selected_mode = st.selectbox(
        "é€‰æ‹©å¯¼èˆªæ¨¡å¼:",
        ('shortest', 'fastest', 'balanced'),
        key='mode_select',
        help="Shortest: ä¼˜å…ˆæœ€çŸ­è·¯å¾„; Fastest: ä¼˜å…ˆæœ€å¿«äº¤ä»˜ (é«˜æ—¶é—´æƒ©ç½š); Balanced: å¹³è¡¡è·¯å¾„å’Œæ—¶é—´ã€‚"
    )
with col2:
    timesteps = st.number_input(
        "è®­ç»ƒæ­¥æ•° (Timesteps):",
        min_value=10000,
        max_value=500000,
        value=150000,
        step=10000,
        key='timesteps_input',
        help="å¼ºåŒ–å­¦ä¹  Agent çš„è®­ç»ƒæ—¶é•¿ã€‚æ­¥æ•°è¶Šé«˜ï¼Œå­¦ä¹ æ•ˆæœå¯èƒ½è¶Šå¥½ã€‚"
    )
with col3:
    grid_size = st.number_input(
        "ç½‘æ ¼å¤§å° (Grid Size):",
        min_value=10,
        max_value=30,
        value=20,
        step=5,
        key='grid_size_input',
        help="ç‰©æµç¯å¢ƒçš„ç½‘æ ¼åœ°å›¾å°ºå¯¸ (N x N)."
    )


if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ & éªŒè¯ POUW", use_container_width=True):
    # ç¡®ä¿åœ¨è¿è¡Œä¹‹å‰æ¸…ç†æ‰æ—§çš„ GIFï¼Œé¿å…ç¼“å­˜é—®é¢˜
    if os.path.exists(f"navigation_{selected_mode}.gif"):
        os.remove(f"navigation_{selected_mode}.gif")
        
    st.markdown("### è®­ç»ƒæ—¥å¿—")
    log_container = st.empty()
    
    with st.spinner(f"æ­£åœ¨è®­ç»ƒ {selected_mode.upper()} æ¨¡å¼... è¯·ç­‰å¾…..."):
        # è®­ç»ƒ Agent
        with st.empty():
            model, training_logs = train_agent(selected_mode, timesteps, grid_size)

        # è¿è¡Œè¯„ä¼°
        # é”™è¯¯å‘ç”Ÿåœ¨è¿™é‡Œï¼Œä½†é—®é¢˜åœ¨ render æ–¹æ³•ä¸­
        test_result, gif_path = run_test_and_render(model, selected_mode, grid_size) 

        # è®°å½• POUW
        user_params = {"mode": selected_mode, "total_timesteps": timesteps, "grid_size": grid_size}
        pouw_record = save_pouw_to_blockchain(user_params, training_logs, test_result, model)
    
    # ---------------------------------
    # ç»“æœå±•ç¤º
    # ---------------------------------
    st.success(f"è®­ç»ƒå’ŒéªŒè¯å®Œæˆï¼æ¨¡å¼: {selected_mode.upper()}")
    
    st.markdown("---")
    
    col_res1, col_res2 = st.columns(2)
    with col_res1:
        st.subheader("âœ… å¯¼èˆªç»“æœ")
        st.metric("æœ€ç»ˆæ­¥æ•°", test_result['steps'])
        st.metric("æ€»å¥–åŠ±", f"{test_result['total_reward']:.2f}")
        st.metric("åˆ°è¾¾ç›®æ ‡", "æ˜¯" if test_result['reach_goal'] else "å¦")
        
        # æ˜¾ç¤º GIF
        if os.path.exists(gif_path):
            st.image(gif_path, caption=f"ç‰©æµ Agent å¯¼èˆªè·¯å¾„ ({selected_mode} æ¨¡å¼)")

    with col_res2:
        st.subheader("ğŸ”— POUW åŒºå—é“¾è®°å½•")
        st.metric("æ–°åŒºå—ç´¢å¼•", pouw_record['block_index'])
        st.metric("é“¾æœ‰æ•ˆæ€§", pouw_record['is_chain_valid'], 
                  delta="é“¾éªŒè¯å¤±è´¥ï¼Œå¯èƒ½æŒ–çŸ¿éš¾åº¦è¿‡é«˜æˆ–æ•°æ®æ ¡éªŒé—®é¢˜" if not pouw_record['is_chain_valid'] else "é“¾éªŒè¯æˆåŠŸ",
                  delta_color="inverse")
        st.write("åŒºå—å“ˆå¸Œ:", pouw_record['block_hash'][:12] + "...")
        with st.expander("æŸ¥çœ‹å®Œæ•´çš„ POUW æ•°æ®"):
            st.json(pouw_record['data'])
            
    st.markdown("---")
    st.subheader("ğŸŒ å®Œæ•´åŒºå—é“¾çŠ¶æ€")
    st.json([block.to_dict() for block in st.session_state.rl_pouw_chain.chain])
